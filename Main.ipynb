{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - What movie to watch tonight?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collection of data\n",
    "In order to collect the data for the homework I performed 2 steps:\n",
    "* I **parsed** the movies1.html file, obtaining a list of Wikipedia urls to crawl. To parse the file I used the Beautiful Soup library, searching for all the 'a' tags in the file and extracting their 'href' attribute in a list;\n",
    "* Then I **downloaded** the movies from the urls. As suggested in homework text, I saved each downloaded file as 'article_i.html', where 'i' was the number of already downloaded files. So url with Id = 1 in the movie list was saved as article_0, and so on. In order to be able to manage an eventual stop of the process and to resume the process from the last downloaded movie, I used the number of movies already downloaded as starting index of the 'for' cycle. To keep the numbers aligned, I had to save a file even in case of error: they can be recognized by the \\_FILE_NOT_FOUND suffix.\n",
    "\n",
    "Analyzing the error files, I found that 2 of the urls provided in the movies1.html files were incorrect, and received a 404 (Page Not Found) response from the server. They were:\n",
    "* Id 9430 - url https://en.wikipedia.org/wiki/A_Distant_Thunder_(1978_film)\n",
    "* Id 9672 - url https://en.wikipedia.org/wiki/Image_of_the_Beast_(film)\n",
    "\n",
    "I downloaded a total of **9998** movies files, for a total dimension of about 608 MB. Please see movies_properties.jps image in repository. \n",
    "The code is available in collector.py file in repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing of data\n",
    "In parser.py there is the code I used to parse the html files downloaded from Wikipedia. Again, I used Beautiful Soup library to parse the html. \n",
    "* To grab the **title** of the page, I just looked for the 'title' tag.\n",
    "* To grab the **intro**, I selected the div with class 'mw-parser-output', and then, inside that div, I selected the first paragraph without the class 'mw-empty-elt', via the pseudo-selector 'p:not(.mw-empty-elt)'. Then I looped with a 'while' cycle on the next-siblings with name \"None\" or \"p\", concatenating their text content.\n",
    "* To grab the **plot**, I selected the first span element with an 'id' attribute beginning with the text 'Plot' (using the css selector 'span\\[id^=Plot\\]'), in order to identify both \"Plot\" and \"Plot Summary\" sections. \n",
    "\n",
    "Afterwards, I preprocessed the intro and plot sections, using nltk library. For both intro and plot:\n",
    "* I deleted newlines '\\n';\n",
    "* I deleted **punctuation**;\n",
    "* I **tokenized** the text;\n",
    "* I deleted **stopwords**;\n",
    "* I **stemmed** the text using nltk PorterStemmer.\n",
    "\n",
    "The final result was saved in .tsv files with name article_i.tsv.\n",
    "\n",
    "The code is available in parser.py and parser-util.py.\n",
    "\n",
    "During this step I found html files that could not be parsed: in that case I produces an error file. I encountered 382 errors; it could be a further step to analyze the errors to improve the parsing procedure and/or the movie list. In the repository you can find screenshots of the error files (parsing_error_1.png and parsing_error_2.png). In the end, I parsed 9618 files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of vocabulary\n",
    "The next step was the creation of a **vocabulary**. For each parsed .tsv file, I examined the second line (the one with content), and considered intro and plot sections. For each word in them, I added an entry to a dictionary if the word was not already in the dictionary. I used the word itself as key, and an incremental integer as value. The final result in saved in json format in **vocabulary.json**. The code regarding this part can be found in vocabulary.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of simple inverted index and tfIdf inverted index\n",
    "In index.py there is the code I used to create a simple inverted index, and a term frequency - inverted document frequency inverted index. The indexes are stored in json format: index.json and tfIdfIndex.json, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution of the query\n",
    "In main.py there is the code that can be executed to perform a query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic exercise - Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of the algorithm for exercise 4 can be found in exercise_4.py.\n",
    "\n",
    "The goal of the algorithm is **to find the longest palindrome substring of a given string**.\n",
    "The strategy I adopted is to look for palindromes in substring created subtracting 0, 1, 2... L characters to the initial string, where L is the length of the string given in input. \n",
    "I used the 'combinations' method from itertools module to calculate **all** of the combinations obtained choosing k characters from a given string, where k = L - i was a decreasing number from L to 0 (actually 0 can never be reached, because the worst case is a palindrome of length 1). The iterable returned combinations is converted to a list of word.\n",
    "Then, to remove duplicates, a set is created from the list of substrings.\n",
    "\n",
    "With the help of two methods, one that reverses a string, and another one that checks whether a string is a palindrome, the algorithm checks if a palindrome is found. In that case, the length is printed out, togheter with the palindrome just found.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, insert the input string: dataminingsapienza\n",
      "Length of longest palindrome:  7\n",
      "Palindrome:  anipina\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations \n",
    "\n",
    "def findCombinations(str, i): \n",
    "    L = len(str) \n",
    "    combs = combinations(str, L - i)\n",
    "    combsList = [''.join(comb) for comb in combs]\n",
    "    return set(combsList)\n",
    "\n",
    "def isPalindrome(word):\n",
    "    return(word == reverse(word))    \n",
    "\n",
    "def reverse(s): \n",
    "    str = \"\" \n",
    "    for i in s: \n",
    "        str = i + str\n",
    "    return str\n",
    "\n",
    "inputString = input(\"Please, insert the input string: \")\n",
    "n = len(inputString)\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    combs = findCombinations(inputString, i)\n",
    "    \n",
    "    for word in combs:\n",
    "        if isPalindrome(word):\n",
    "            print(\"Length of longest palindrome: \", str(n-i))\n",
    "            print(\"Palindrome: \", word)\n",
    "            break\n",
    "    else:\n",
    "        continue\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
